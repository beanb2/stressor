---
title: "stressor"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{stressor}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This package is designed to allow the user to use multiple machine learning 
methods by calling simple commands for data exploration. To get started, this 
package requires that you have <a href = https://www.python.org/downloads/release/python-3810/>Python 3.8.10</a>
installed on your computer. Visit the link before and follow the instructions to
install Python. 

Once installed, we can add the installed stressor package to our work space.
```{r setup}
library(stressor)
```

## Data Generation

In this section we will show an example of generating linear data with a epsilon
and intercept that we choose. We will generate 500 entries to a
linear model that has five variables with equal weights, y-intercept of 0, and a
response standard deviation of 1.
```{r}
set.seed(43421)
lm_data <- data_gen_lm(500, weight_vec = rep(1, 5), y_int = 0, resp_sd = 1)
head(lm_data)
```

## Machine Learning Model Workflow

In this section we will demonstrate the workflow to use this package to explore
the machine learning models(mlm) that are accessible through the PyCaret module
in python. First we need to create a virtual environment for the PyCaret module
to exist in. The first time you run this code it will take a bit as it needs to 
install the necessary modules into the virtual environment. Once installed the 
following message will be shown after you execute the code indicating that you
are now using the virtual environment. See the <a href=#troubleshoot>
troubleshoot</a> section if other errors appear.
```{r}
create_virtualenv()
```

Next we need create all the mlm by running this command. This may take a moment
the first time you run as the PyCaret module needs to be imported. Then
depending on your data size it may take a moment to fit the data. Note you will
be prompted
```{r eval=FALSE}
mlm_lm <- mlm_regressor(Y ~ ., lm_data)
```

```{r echo=FALSE}
mlm_lm <- mlm_regressor(Y ~ ., lm_data, silent = TRUE)
top_rmse <- min(mlm_lm$pred_accuracy$rmse)
```

Now we can look at the initial predictive accuracy of the various models by
```{r}
mlm_lm$pred_accuracy
```

In comparison we can fit this data using the `lm()` function and check the 
initial predictive accuracy with a simple test data.
```{r}
test_index <- sample(1:nrow(lm_data), 50)
test <- lm_data[test_index, ]
train <- lm_data[-test_index, ]
lm_test <- lm(Y ~ ., train)
lm_rmse <- sqrt(sum((lm_test$residuals)^2)/ nrow(train))
```

As we look at this initial result we see that there are some comparable models 
to the rmse of generated from `lm()` which is `r lm_rmse` compared to 
`r top_rmse`. We see that the analytical model is still performs better than the
models that were fitted, however, this can be by chance of the random subsetting
of the test and train set. A better practice will be performing a cross 
validation either Leave One Out (LOO) or standard cross validation. In this case
we will demonstrate the use of standard cross validation as the rest stem from
specifying parameters inside the `cv()` function.
```{r}
mlm_cv <- cv(mlm_lm, lm_data, n_folds = 10, k_mult = NULL)
lm_cv <- cv(lm_test, lm_data, n_folds = 10, k_mult = NULL)
rmse(mlm_cv, lm_data$Y)
rmse(lm_cv, lm_data$Y)
```
We can see that linear model fitted by `lm()` is still better than the mlm.

## Real Data Example
We will be using the Boston Housing Data that has been updated, we will also be
ignoring spatial autocorrelation. The purpose of this data set is to show it's 
use on the functions in this package. Specifically the cross validation 
```{r eval=FALSE}
data(boston)
mlm_boston <- mlm_regressor(cmedv ~ ., boston)
```

```{r echo=FALSE}
data(boston)
mlm_boston <- mlm_regressor(cmedv ~ ., boston, silent = TRUE)
mlm_boston$pred_accuracy
```
Observe the initial rmse values for the Boston data set. Now let's compare these
to the cross validated rmse values.
```{r}
mlm_boston_cv <- cv(mlm_boston, boston, n_folds = 10)
mlm_boston_rmse <- rmse(mlm_boston_cv, boston$cmedv)
mlm_boston_rmse
```
Now how about compared to the clustered cross validation
```{r}
mlm_boston_clust_cv <- cv(mlm_boston, boston, n_folds = 10, k_mult = 5)
mlm_boston_clust_rmse <- rmse(mlm_boston_clust_cv, boston$cmedv)
mlm_boston_clust_rmse
```


<div id="troubleshoot">
## Troubleshooting
When instantiating the virtual environment you may receive some errors or 
warnings. `reticulate` has done a nice job with the error handling of 
instantiating.
```{r error=TRUE}
create_virtualenv()
```

</div>
